# Toolzz Search ğŸ”ğŸ¤–

Sistema de scraping inteligente que combina **Playwright** (navegador headless) com **GPT-5 mini** (visÃ£o + texto) para extrair dados estruturados de qualquer pÃ¡gina web.

---

## Como Funciona

```
URL + Prompt do UsuÃ¡rio
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Playwright Browser â”‚  â† Stealth mode, anti-bot, smart scroll
â”‚   (Chromium headless) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚  Screenshot + HTML + Texto + Accessibility Tree + Image URLs
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GPT-5 mini (IA)    â”‚  â† VisÃ£o multimodal + contexto semÃ¢ntico
â”‚   ExtraÃ§Ã£o com schema â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚  JSON estruturado
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ValidaÃ§Ã£o + SQLite â”‚  â† PersistÃªncia automÃ¡tica
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pipeline completo:**
1. O usuÃ¡rio informa uma **URL** e um **prompt** descrevendo o que quer extrair.
2. O **Playwright** navega atÃ© a pÃ¡gina com stealth (anti-detecÃ§Ã£o de bot).
3. O **Smart Scroll** rola a pÃ¡gina detectando carregamento dinÃ¢mico.
4. O browser captura: **screenshot**, **HTML**, **texto renderizado**, **Ã¡rvore de acessibilidade** e **URLs de imagens**.
5. Tudo Ã© enviado ao **GPT-5 mini** que extrai dados estruturados em JSON.
6. Os dados sÃ£o **validados** contra um schema Pydantic.
7. O resultado Ã© **salvo no SQLite** e retornado ao frontend.

---

## Requisitos

- Python 3.11+
- Node.js 18+
- Chave de API da OpenAI (`OPENAI_API_KEY`)

---

## Setup RÃ¡pido

### Backend

```bash
# Criar ambiente virtual
python -m venv venv

# Ativar (Windows)
venv\Scripts\activate

# Instalar dependÃªncias
pip install -r requirements.txt

# Instalar navegador do Playwright
playwright install chromium

# Configurar variÃ¡veis de ambiente
copy .env.example .env
# Edite .env e configure OPENAI_API_KEY
```

### Frontend

```bash
cd frontend
npm install
copy .env.example .env
```

---

## Rodar

### Backend (API)

```bash
python run_backend.py
```

- API: `http://127.0.0.1:8000`
- Docs (Swagger): `http://127.0.0.1:8000/docs`
- Health check: `http://127.0.0.1:8000/health`

### Frontend (React + Vite)

```bash
cd frontend
npm run dev
```

- Interface: `http://127.0.0.1:5173`

---

## Formatos de SaÃ­da

O sistema oferece **3 formatos** que alteram como a IA processa e retorna os dados:

| Formato | Campo `summary` | Campo `findings` | Uso ideal |
|---|---|---|---|
| **Lista** | Contexto breve | Muitos itens estruturados (tÃ­tulo, descriÃ§Ã£o, URL, extras) | Produtos, links, contatos |
| **Resumo** | ParÃ¡grafo denso com pontos-chave em **negrito** | Poucos tÃ³picos cruciais | VisÃ£o geral rÃ¡pida |
| **RelatÃ³rio Completo** | Documento Markdown rico com tÃ­tulos, listas e imagens | ExtraÃ§Ã£o extensa e analÃ­tica | AnÃ¡lise profunda |

### Exemplo de uso

1. **Lista**: "Liste os 10 principais produtos com nome, preÃ§o e link"
2. **Resumo**: "Resuma o conteÃºdo principal desta pÃ¡gina"
3. **RelatÃ³rio**: "FaÃ§a uma anÃ¡lise completa desta pÃ¡gina com todos os detalhes"

No modo **RelatÃ³rio Completo**, a IA inclui automaticamente as imagens encontradas na pÃ¡gina dentro do texto Markdown.

---

## OtimizaÃ§Ãµes do Browser

O sistema implementa diversas otimizaÃ§Ãµes para performance e stealth:

### ğŸ›¡ï¸ Anti-DetecÃ§Ã£o (Stealth)
- Remove `navigator.webdriver`
- Define `navigator.languages`, `platform`, `hardwareConcurrency`
- Injeta `window.chrome.runtime`
- User-Agent realista (Chrome 133, Windows)
- Headers `Accept-Language`, `Sec-CH-UA-Platform`

### âš¡ Resource Blocking
- Bloqueia carregamento de **imagens**, **fontes** e **mÃ­dia** por padrÃ£o
- Economiza banda e acelera carregamento
- DesativÃ¡vel por estratÃ©gia de fallback

### ğŸ“œ Smart Scroll
- Rola a pÃ¡gina incrementalmente
- Detecta quando o conteÃºdo parou de carregar (compara `scrollHeight`)
- Aguarda `networkidle` entre scrolls
- Para automaticamente quando atinge o fim da pÃ¡gina

### ğŸ’¾ Session Persistence
- Salva cookies e storage state por domÃ­nio em `data/sessions/`
- Reutiliza sessÃµes em scrapes futuros do mesmo domÃ­nio
- Ãštil para sites que lembram preferÃªncias ou aceitaÃ§Ã£o de cookies

### ğŸŒ³ Accessibility Tree
- Captura a Ã¡rvore de acessibilidade do Chrome
- Envia para a IA como contexto semÃ¢ntico (muito menor que HTML bruto)
- Reduz tokens consumidos e melhora precisÃ£o

### ğŸ–¼ï¸ Image URL Extraction
- Extrai URLs de atÃ© 50 imagens da pÃ¡gina
- As URLs sÃ£o passadas para a IA
- No modo **RelatÃ³rio**, a IA inclui imagens relevantes no texto Markdown

---

## API Endpoints

### `POST /api/scrape`

Executa um scraping completo.

```json
{
  "url": "https://example.com",
  "schema": "guided_extract",
  "prompt": "generic",
  "user_prompt": "Liste os produtos com nome e preÃ§o",
  "output_format": "list",
  "wait_until": "networkidle",
  "timeout": 30000,
  "screenshot_quality": 70,
  "full_page": false,
  "auto_scroll": true,
  "scroll_steps": 6
}
```

**Resposta:**

```json
{
  "success": true,
  "data": {
    "summary": "Resumo gerado pela IA...",
    "findings": [
      {
        "title": "Produto X",
        "description": "DescriÃ§Ã£o...",
        "url": "https://...",
        "extra": { "preco": "99.90" }
      }
    ]
  },
  "metadata": {
    "url": "https://example.com",
    "model_used": "gpt-5-mini-2025-08-07",
    "tokens_used": { "prompt": 1500, "completion": 300, "total": 1800 },
    "cost_usd": 0.0012,
    "duration_seconds": 8.5,
    "quality": { "quality_score": 0.95 }
  },
  "record_id": 42
}
```

### `GET /api/history`

Consulta histÃ³rico de scrapes salvos no SQLite.

| ParÃ¢metro | Tipo | DescriÃ§Ã£o |
|---|---|---|
| `limit` | int | Quantidade de registros (padrÃ£o: 20) |
| `success` | bool | Filtrar por sucesso/falha |
| `domain` | string | Filtrar por domÃ­nio |

### `GET /health`

Retorna `{"status": "ok"}` se o backend estÃ¡ rodando.

---

## VariÃ¡veis de Ambiente (.env)

```env
# ObrigatÃ³ria
OPENAI_API_KEY=sk-...

# Opcionais (valores padrÃ£o mostrados)
OPENAI_MODEL=gpt-5-mini-2025-08-07
HEADLESS=true
BROWSER_TIMEOUT=30000
VIEWPORT_WIDTH=1920
VIEWPORT_HEIGHT=1080
MAX_CONCURRENT_TASKS=3
RETRY_ATTEMPTS=3
RETRY_DELAY=2
DATABASE_URL=sqlite+aiosqlite:///./data/scraper_data.db
LOG_LEVEL=INFO
```

---

## Estrutura do Projeto

```
toolzz-search/
â”œâ”€â”€ run_backend.py              # Entry point do servidor
â”œâ”€â”€ requirements.txt            # DependÃªncias Python
â”œâ”€â”€ .env                        # VariÃ¡veis de ambiente
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ settings.py         # ConfiguraÃ§Ãµes (Pydantic Settings)
â”‚   â”‚   â””â”€â”€ prompts.py          # System prompts da IA
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ browser.py          # Playwright: stealth, scroll, captura
â”‚   â”‚   â”œâ”€â”€ ai_processor.py     # GPT-5 mini: extraÃ§Ã£o multimodal
â”‚   â”‚   â”œâ”€â”€ orchestrator.py     # Pipeline: browser â†’ IA â†’ validaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ validator.py        # ValidaÃ§Ã£o de dados extraÃ­dos
â”‚   â”‚   â”œâ”€â”€ storage.py          # PersistÃªncia SQLite + JSON
â”‚   â”‚   â””â”€â”€ errors.py           # ExceÃ§Ãµes customizadas
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py          # Schemas Pydantic (GuidedExtract, etc.)
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ helpers.py          # clean_html, utilitÃ¡rios
â”‚   â”‚   â”œâ”€â”€ cost_tracker.py     # CÃ¡lculo de custo por request
â”‚   â”‚   â””â”€â”€ logger.py           # ConfiguraÃ§Ã£o do Loguru
â”‚   â”‚
â”‚   â””â”€â”€ web/
â”‚       â””â”€â”€ main.py             # FastAPI: rotas /api/scrape, /api/history
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx             # Componente principal React
â”‚   â”‚   â””â”€â”€ index.css           # Estilos globais
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ scraper_data.db         # SQLite (gerado automaticamente)
â”‚   â”œâ”€â”€ sessions/               # Cookies salvos por domÃ­nio
â”‚   â””â”€â”€ screenshots/            # Screenshots salvos
â”‚
â””â”€â”€ logs/
    â””â”€â”€ scraper.log             # Logs estruturados (Loguru)
```

---

## Limites Conhecidos

- **Anti-bot agressivo**: WAFs como Cloudflare Challenge, CAPTCHAs visuais e Incapsula podem bloquear.
- **Login obrigatÃ³rio**: ConteÃºdo atrÃ¡s de autenticaÃ§Ã£o nÃ£o Ã© acessÃ­vel automaticamente.
- **Sites muito pesados**: PÃ¡ginas com muitos MBs de JavaScript podem causar timeout.
- **Rate limiting**: Scrapes frequentes no mesmo domÃ­nio podem ser bloqueados.

O sistema detecta automaticamente bloqueios (CAPTCHA, 403, 429) e reporta no resultado.

---

## Testes

```bash
pytest tests/
```
